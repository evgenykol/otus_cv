{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно каким-то образом загрузить данные. И попробовать нарисовать на них bouding box-ы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 720, 1280]) 100 <class 'dict'>\n",
      "1310  - 1310.jpeg\n",
      "[[147.6285577889684, 255.99795244419116, 81.82212034100837, 152.63871900698197], [871.8487127118674, 313.4508504284022, 130.96409851161883, 221.54453004544163], [1126.18748843893, 244.62134911372516, 84.13255687486667, 198.16794911740126], [621.0183320115026, 276.46175937213104, 91.12088758939889, 208.38885758656093], [668.0268583822732, 356.5367737878781, 133.98136663257037, 241.27491512951957], [686.1048754181467, 247.97988138642003, 86.32852027329228, 182.28439135598308], [808.638884818095, 218.1205706183819, 73.33930536223944, 174.99381857469814], [511.2910386499924, 182.4466164592711, 72.20012244379666, 144.90390676246537], [336.4641105980184, 231.0954725183006, 78.18405010030374, 144.8020613107186], [544.848820300097, 206.27356823646772, 69.27584341248624, 122.79919267768803]] tensor([[ 147.6286,  255.9980,   81.8221,  152.6387],\n",
      "        [ 871.8487,  313.4508,  130.9641,  221.5445],\n",
      "        [1126.1875,  244.6214,   84.1326,  198.1680],\n",
      "        [ 621.0183,  276.4618,   91.1209,  208.3889],\n",
      "        [ 668.0269,  356.5368,  133.9814,  241.2749],\n",
      "        [ 686.1049,  247.9799,   86.3285,  182.2844],\n",
      "        [ 808.6389,  218.1206,   73.3393,  174.9938],\n",
      "        [ 511.2910,  182.4466,   72.2001,  144.9039],\n",
      "        [ 336.4641,  231.0955,   78.1841,  144.8021],\n",
      "        [ 544.8488,  206.2736,   69.2758,  122.7992]])\n",
      "tensor([[[ 147.6286,  255.9980,  229.4507,  408.6367],\n",
      "         [ 871.8487,  313.4508, 1002.8128,  534.9954],\n",
      "         [1126.1875,  244.6214, 1210.3201,  442.7893],\n",
      "         [ 621.0183,  276.4618,  712.1392,  484.8506],\n",
      "         [ 668.0269,  356.5368,  802.0082,  597.8117],\n",
      "         [ 686.1049,  247.9799,  772.4333,  430.2643],\n",
      "         [ 808.6389,  218.1206,  881.9781,  393.1144],\n",
      "         [ 511.2910,  182.4466,  583.4911,  327.3505],\n",
      "         [ 336.4641,  231.0955,  414.6482,  375.8975],\n",
      "         [ 544.8488,  206.2736,  614.1246,  329.0728]]])\n",
      "tensor([[[ 27,  27,  27,  ...,  38,  38,  38],\n",
      "         [ 27,  27,  27,  ...,  38,  38,  38],\n",
      "         [ 27,  27,  27,  ...,  38,  38,  38],\n",
      "         ...,\n",
      "         [196, 198, 198,  ..., 233, 233, 231],\n",
      "         [198, 198, 197,  ..., 226, 226, 225],\n",
      "         [198, 198, 197,  ..., 227, 227, 227]],\n",
      "\n",
      "        [[  5,   5,   5,  ...,  38,  38,  38],\n",
      "         [  5,   5,   5,  ...,  38,  38,  38],\n",
      "         [  5,   5,   5,  ...,  38,  38,  38],\n",
      "         ...,\n",
      "         [140, 142, 142,  ..., 187, 187, 188],\n",
      "         [142, 142, 143,  ..., 179, 179, 180],\n",
      "         [142, 142, 143,  ..., 180, 180, 180]],\n",
      "\n",
      "        [[ 70,  70,  70,  ...,  48,  48,  48],\n",
      "         [ 70,  70,  70,  ...,  48,  48,  48],\n",
      "         [ 70,  70,  70,  ...,  48,  48,  48],\n",
      "         ...,\n",
      "         [ 55,  57,  57,  ..., 109, 109, 109],\n",
      "         [ 57,  57,  57,  ...,  99,  99,  99],\n",
      "         [ 57,  57,  57,  ..., 100, 100, 100]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Boxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mprint\u001b[39m(bboxes_tensor)\n\u001b[0;32m     61\u001b[0m \u001b[39mprint\u001b[39m(tens)\n\u001b[1;32m---> 63\u001b[0m img \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mdraw_bounding_boxes(tens, bboxes_tensor, width\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, colors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgreen\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     64\u001b[0m img \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToPILImage()(img)\n\u001b[0;32m     65\u001b[0m img\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\utils.py:202\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[1;34m(image, boxes, labels, colors, fill, width, font, font_size)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly grayscale and RGB images are supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m \u001b[39melif\u001b[39;00m (boxes[:, \u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m boxes[:, \u001b[39m2\u001b[39m])\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m (boxes[:, \u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m boxes[:, \u001b[39m3\u001b[39m])\u001b[39m.\u001b[39many():\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBoxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    206\u001b[0m num_boxes \u001b[39m=\u001b[39m boxes\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m num_boxes \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Boxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import natsort\n",
    "\n",
    "bboxes = []\n",
    "# load json\n",
    "with open('team_classification_data\\\\bboxes.json') as bboxes_json:\n",
    "    bboxes = json.load(bboxes_json)\n",
    "\n",
    "# load images \n",
    "data_dir = 'team_classification_data\\\\frames\\\\'\n",
    "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.jpeg']\n",
    "sorted_filenames = natsort.natsorted(filenames)\n",
    "\n",
    "image_tensors = [torchvision.io.read_image(data_dir + name) for name in sorted_filenames]\n",
    "batch = torch.stack(image_tensors)\n",
    "print(batch.shape, len(bboxes), type(bboxes))\n",
    "\n",
    "# iterate over images an draw corresponding bboxes\n",
    "# for key, value in bboxes.items():\n",
    "#    print(key, value)\n",
    "\n",
    "for bb, fn, tens in zip(bboxes.items(), sorted_filenames, image_tensors):\n",
    "    print(f'{bb[0]}  - {fn}')\n",
    "\n",
    "    bboxes_list = []\n",
    "    for player_id, bbox in bb[1].items():\n",
    "        # print(f'id: {player_id} - team: {bbox[\"team\"]} - bbox: {bbox[\"box\"]}')\n",
    "        # print(tens.shape) = torch.Size([3 = rgb, 720 = height, 1280 = width]) \n",
    "        height = tens.shape[1]\n",
    "        width = tens.shape[2]\n",
    "\n",
    "        x = bbox['box'][0] * width\n",
    "        y = bbox['box'][1] * height\n",
    "        w = bbox['box'][2] * width\n",
    "        h = bbox['box'][3] * height\n",
    "\n",
    "        bboxes_list.append([x, y, w, h])\n",
    "\n",
    "        # bbox_tensor = torch.tensor([x, y, w, h])\n",
    "        # bbox_tensor = bbox_tensor.unsqueeze(0)\n",
    "\n",
    "\n",
    "        # bbox_tensor. = torchvision.ops.box_convert(bbox_tensor, 'xywh', 'xyxy')\n",
    "\n",
    "\n",
    "        # color = 'green'\n",
    "        # if bbox['team'] == 0 :\n",
    "        #     color = 'red'\n",
    "        # img = utils.draw_bounding_boxes(tens, bbox_tensor, width=5, colors=color)\n",
    "\n",
    "        # if player_id > '4':\n",
    "        #     img = transforms.ToPILImage()(img)\n",
    "        #     img.show()\n",
    "    \n",
    "    bboxes_tensor = torch.tensor(bboxes_list)\n",
    "    print(bboxes_list, bboxes_tensor)\n",
    "    bboxes_tensor = bboxes_tensor.unsqueeze(0)\n",
    "    bboxes_tensor = torchvision.ops.box_convert(bboxes_tensor, 'xywh', 'xyxy')\n",
    "    print(bboxes_tensor)\n",
    "    print(tens)\n",
    "\n",
    "    img = utils.draw_bounding_boxes(tens, bboxes_tensor, width=2, colors='green')\n",
    "    img = transforms.ToPILImage()(img)\n",
    "    img.show()\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
